ğŸ“Œ Description du Projet:
Ce projet vise Ã  entraÃ®ner un agent autonome Ã  naviguer dans un environnement 3D simulÃ© sous Unity, dans le but d'atteindre un drapeau cible tout en Ã©vitant des obstacles. Le projet a Ã©tÃ© conÃ§u pour illustrer lâ€™application de lâ€™apprentissage par renforcement profond dans un environnement interactif.

RÃ©alisÃ© par :
Hajar Abdessadek,
Amal Lastak et
Salima Khalil El Bouni

ğŸ¯ Objectif
Lâ€™objectif principal est dâ€™implÃ©menter et entraÃ®ner un agent pour quâ€™il apprenne Ã  se dÃ©placer intelligemment vers un objectif final en :
Ã©vitant les obstacles prÃ©sents dans son environnement,
choisissant des trajectoires optimales via lâ€™apprentissage automatique,
sâ€™adaptant Ã  des environnements variÃ©s.

âš™ï¸Technologies et Outils UtilisÃ©s:
Unity ML-Agents : UtilisÃ© comme environnement de simulation 3D et interface pour communiquer avec le modÃ¨le dâ€™apprentissage Python.

Python : Langage de programmation utilisÃ© pour coder l'algorithme PPO et gÃ©rer l'entraÃ®nement.

TensorFlow : Framework de deep learning permettant de construire et dâ€™entraÃ®ner les rÃ©seaux de neurones de lâ€™agent.

Visual Studio Code : Environnements de dÃ©veloppement pour coder, tester et visualiser les performances de lâ€™agent.

ğŸ§  Algorithmes UtilisÃ©s
ğŸ“Œ PPO (Proximal Policy Optimization)

Nous avons choisi PPO, un algorithme robuste de policy gradient, pour entraÃ®ner notre agent. PPO amÃ©liore la stabilitÃ© de lâ€™entraÃ®nement grÃ¢ce Ã  une rÃ©gularisation de la mise Ã  jour de la politique, ce qui permet :

des mises Ã  jour efficaces de la politique,

dâ€™Ã©viter les mises Ã  jour trop agressives (grÃ¢ce Ã  une fonction de perte "clippÃ©e"),

un bon compromis entre exploration et exploitation.

ğŸ“Œ Actor-Critic

Notre architecture repose sur le paradigme Actor-Critic :

Actor : gÃ©nÃ¨re les actions Ã  effectuer (politique).

Critic : estime la valeur de lâ€™Ã©tat (fonction de valeur), permettant Ã  lâ€™Actor de mieux sâ€™ajuster.

Lâ€™agent apprend ainsi Ã  amÃ©liorer progressivement ses choix dâ€™actions en fonction des retours du Critic.


ğŸ® AperÃ§u de lâ€™Environnement de Simulation

Lâ€™environnement a Ã©tÃ© conÃ§u dans Unity afin de simuler un agent qui doit atteindre un drapeau tout en Ã©vitant des obstacles.

Espace d'observation (ou Ã©tat) :
Lâ€™agent reÃ§oit des observations sous forme vectorisÃ©e (au lieu dâ€™une image), ce qui permet une reprÃ©sentation plus lÃ©gÃ¨re et adaptÃ©e au traitement par rÃ©seau de neurones.

Espace dâ€™action :
Continu, ce qui signifie que les actions ne sont pas limitÃ©es Ã  un ensemble discret mais peuvent prendre des valeurs rÃ©elles. Cela permet un meilleur contrÃ´le de la navigation.

Forme de lâ€™action :(nombre d'agents, 2)
Dans notre cas, il y a un seul agent actif Ã  chaque Ã©tape, donc la forme est (1, 2) (correspondant par exemple aux actions de mouvement selon deux axes).

SystÃ¨me de rÃ©compense :

+2 : Lâ€™agent atteint la cible (le drapeau).

(1.0 / MaxStep) : RÃ©compense lÃ©gÃ¨re Ã  chaque Ã©tape pour encourager la progression vers lâ€™objectif.

(1.0 / MaxStep) Ã©galement en cas de collision avec un mur (aucune pÃ©nalitÃ© spÃ©cifique, mais pas dâ€™incitation Ã  foncer sur les obstacles non plus).

MaxStep : DÃ©termine la durÃ©e maximale dâ€™un Ã©pisode. Si lâ€™agent nâ€™atteint pas lâ€™objectif avant ce nombre d'Ã©tapes, lâ€™environnement se rÃ©initialise.

 
 ğŸ” Fonctionnement du ModÃ¨le


1_Lâ€™agent observe lâ€™environnement (positions relatives du drapeau, obstaclesâ€¦).

2_Il choisit une action (avancer, tournerâ€¦) via la politique (Actor).

3_Il reÃ§oit une rÃ©compense selon lâ€™impact de son action.

4_Le Critic estime la qualitÃ© de lâ€™action.

5_Les poids du rÃ©seau sont mis Ã  jour via PPO.

6_Le processus se rÃ©pÃ¨te sur plusieurs Ã©pisodes jusquâ€™Ã  convergence.

ğŸ› ï¸ Comment l'exÃ©cuter
CrÃ©er et activer l'environnement virtuel Python : (version Python utilisÃ©e - 3.8.x):

$ python -m venv myvenv

$ myvenv\Scripts\activate

Installer les dÃ©pendances : (vÃ©rifiez les versions exactes des dÃ©pendances dans le fichier requirements.txt )

(myvenv) $ pip install -e ./ml-agents/ml-agents-envs

(myvenv) $ pip install tensorflow

(myvenv) $ pip install keras

(myvenv) $ pip install tensorboardX

Maintenant, pour dÃ©marrer le processus de formation, utilisez les commandes suivantes :

(myvenv) $ cd PPO-algo-with-custom-Unity-environment

(myvenv) $ python train.py

Vue de lâ€™environnement simulÃ© avec obstacles et objectif:

![Capture d'Ã©cran 2025-06-06 150644](https://github.com/user-attachments/assets/34317cad-3b2c-4855-98d0-cc04ed280dcc)


